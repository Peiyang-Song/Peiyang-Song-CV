%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% "ModernCV" CV and Cover Letter
% LaTeX Template
% Version 1.11 (19/6/14)
%
% This template has been downloaded from: 
% http://www.LaTeXTemplates.com
%
% Original author: Xavier Danaux (xdanaux@gmail.com)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
%
% Important note:
% This template requires the moderncv.cls and .sty files to be in the same 
% directory as this .tex file. These files provide the resume style and themes 
% used for structuring the document.
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%-------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%-------------------------------------------------------------------

\documentclass[11pt,letterpaper,sans]{moderncv} 
% Font sizes: 10, 11, or 12; 
% paper sizes: a4paper, letterpaper, a5paper, legalpaper, executivepaper or landscape; 
% font families: sans or roman.

\moderncvstyle{classic} % CV theme - options include: 'casual' (default), 'classic', 'oldstyle' and 'banking'
\moderncvcolor{blue} % CV color - options include: 'blue' (default), 'orange', 'green', 'red', 'purple', 'grey' and 'black'

\usepackage{lipsum} % Used for inserting dummy 'Lorem ipsum' text into the template
\usepackage{soul}
\usepackage[scale=0.75]{geometry} % Reduce document margins
\setlength{\hintscolumnwidth}{3cm} % Uncomment to change the width of the dates column
\setlength{\makecvtitlenamewidth}{10cm} % For the 'classic' style, uncomment to adjust the width of the space allocated to your name

%-------------------------------------------------------------------
%	NAME AND CONTACT INFORMATION SECTION
%-------------------------------------------------------------------

\firstname{Peiyang}
\familyname{Song}

\title{}
\address{1200 E California Blvd, Pasadena, CA}{}
\email{psong@caltech.edu}
\homepage{peiyang-song.github.io/}{peiyang-song.github.io/}

%-------------------------------------------------------------------

\begin{document}

\makecvtitle % Print the CV title

%-------------------------------------------------------------------
%	EDUCATION SECTION
%-------------------------------------------------------------------

\section{\textbf{Education}}
\vspace{0.5em}

\cvitem{6/2026}{\textbf{California Institute of Technology} \hfill Pasadena, CA}
\cvitem{}{\textit{B.S. in Computer Science \& Minor in Robotics}}
\cvitem{}{Advisors: Prof. \href{https://netlab.caltech.edu/}{Steven Low} \& Prof. \href{https://scholar.google.com/citations?user=-YP8MJ0AAAAJ&hl=en}{Günter Niemeyer}. GPA: \textbf{4.2/4.0}}

\vspace{1em}

%-------------------------------------------------------------------
%	RESERACH INTERESTS SECTION
%-------------------------------------------------------------------

\section{\textbf{Research Interests}}
\vspace{0.5em}

Machine Learning $\cdot$ Natural Language Processing $\cdot$ Automated Reasoning $\cdot$ Neuro-Symbolic AI

% \vspace{1em}

% My research interest is mainly in machine reasoning, especially AI for mathematics and code generation. In the past, I also worked on energy-efficient machine learning systems and machine translation.

\vspace{1em}

%-------------------------------------------------------------------
%	WORK EXPERIENCE SECTION
%-------------------------------------------------------------------

\section{\textbf{Work Experience}}
\vspace{0.5em}

\cvitem{6/2024 -- Present}{\textbf{Stanford University} \hfill Palo Alto, CA}
\cvitem{}{\textit{Researcher @ \href{https://ai.stanford.edu/}{Stanford AI Lab (SAIL)} and \href{https://cocolab.stanford.edu/}{Computation \& Cognition Lab}}}
\cvitem{}{Advisors: Prof. \href{https://cocolab.stanford.edu/ndg}{Noah Goodman} (Stanford), \href{https://gpoesia.com/}{Gabriel Poesia} (Stanford)}

% \vspace{0.5em}

% \begin{itemize}
%     \item Project 1: \textbf{Lean Inventor: Auto-Expanding Math Knowledge in Neural Theorem Proving} (Lead, Wrapping Up)
%     \vspace{0.5em}
%     \begin{itemize}
%         \item Built the \textbf{first} general framework that both auto-expands math libraries in proof assistants and improves neural prover abilities in an \textbf{autonomous loop}.
%         \item Developed the \textbf{first} conjecturing model based on Lean Mathlib, via a novel hierarchical data extraction method that preserves traces of math library development.
%         \item Employed curiosity-driven methods from cognitive science to improve conjecturing quality.
%     \end{itemize}
%     \vspace{1em}
%     \item Project 2: \textbf{A Survey on LLM Reasoning Failures} (Lead, Wrapping Up).
%     \vspace{0.5em}
%     \begin{itemize}
%         \item Laid the \textbf{first} theoretical foundation of LLM reasoning failures supported by rigorous definitions.
%         \item Presented the \textbf{first comprehensive} survey and analysis of existing LLM reasoning failure cases, with a position that encourages research along this direction and identifies mitigation methods.
%     \end{itemize}
% \end{itemize}

\vspace{1em}

\cvitem{2/2023 -- 2/2025}{\textbf{California Institute of Technology} \hfill Pasadena, CA}
\cvitem{}{\textit{Research Fellow @ \href{http://tensorlab.cms.caltech.edu/users/anima/index.html}{Anima AI+Science Lab}}}
\cvitem{}{Advisors: Prof. \href{https://www.eas.caltech.edu/people/anima}{Anima Anandkumar} (Caltech), Dr. \href{https://yangky11.github.io/}{Kaiyu Yang} (Meta)}

% \vspace{0.5em}

% \begin{itemize}
%     \item Project 1: \textbf{LeanDojo: Theorem Proving with Retrieval-Augmented Language Models} (Main contributor, Published).
%     \vspace{0.5em}
%     \begin{itemize}
%         \item Took charge of building the \textbf{first} well-annotated data extraction and real-time programmatic interaction with the Lean proof assistant; constructed the \textbf{largest} fine-grained benchmark for theorem proving in Lean consisting of \textbf{96,962} theorems and proofs.
%         \item Built the first LLM-based prover that is augmented with retrieval for selecting premises from a vast math library, with performance \textbf{40\%} over state-of-the-art non-retrieval baselines and GPT4.
%         \item Published and \textbf{oral}-presented results at \textbf{NeurIPS 2024}.
%     \end{itemize}
%     \vspace{1em}
%     \item Project 2: \textbf{Towards Large Language Models as Copilots for Theorem Proving in Lean} (Lead, Under Review).
%     \vspace{0.5em}
%     \begin{itemize}
%         \item Built the \textbf{first} open-source framework to run machine learning inferences and applications \textbf{natively} in Lean theorem prover, with the simplest installation burden and usage, fastest runtime, and lowest memory consumption so far.
%         \item Developed the \textbf{best-performing} ML-powered tactic suggestion and \textbf{first-ever} neural proof search \& premise selection tools in Lean, automating \textbf{over 80\%} proof steps from \textit{Mathematics in Lean}.
%         \item Remained the \textbf{most popular} Github repo in Lean with \textbf{over 1k} stars.
%     \end{itemize}
%     \vspace{1em}
%     \item Project 3: \textbf{LeanProgress: Guiding Search for Neural Theorem Proving via Proof Progress Prediction} (Co-Lead, Wrapping Up).
%     \vspace{0.5em}
%     \begin{itemize}
%         \item Constructed the \textbf{first} large-scale dataset of Lean proof search trees, with a data extraction tool generally applicable to any Lean project.
%         \item Created the \textbf{first} proof progress prediction model for theorem proving in Lean to guide development, with a prediction accuracy over \textbf{75\%}.
%         \item Built the \textbf{first} remaining-step-number-prediction tool \textbf{natively} in Lean to aid proof development.
%         \item Integrated the progress prediction model as a reward signal into greedy and RL search, enabling improved performance on the proof generation task on LeanDojo benchmark by \textbf{3.8\%}.
%     \end{itemize}
% \end{itemize}

\vspace{1em}

\cvitem{11/2022 -- 6/2024}{\textbf{University of California, Santa Barbara} \hfill Santa Barbara, CA}
\cvitem{}{\textit{Researcher @ \href{https://www.arch.cs.ucsb.edu/}{Computer Architecture Lab (ArchLab)}}}
\cvitem{}{Advisors: Prof. \href{https://www.arch.cs.ucsb.edu/prof-sherwood}{Timothy Sherwood} (UCSB), Dr. \href{https://dl.acm.org/profile/81100206077}{Jeremy Lau} (Google)}

% \vspace{0.5em}

% \begin{itemize}
%     \item Project 1: \textbf{Energy Efficient Convolution with Temporal Arithmetic} (Co-Lead, Published).
%     \vspace{0.5em}
%     \begin{itemize}
%         \item Established the \textbf{first} complete algebraic space with well-proved theoretical foundation for race logic, a particular temporal logic with high energy-efficiency and unlimited approximation bound.
%         \item Optimized temporal operators for energy efficiency, via tropical algebra and a novel negative log transformation of the traditional binary space into a ``delay space''.
%         \item Built energy-efficient convolution that improves the energy per pixel of each convolution frame by \textbf{more than 2×} compared to the state-of-the-art, while improving the energy delay product by \textbf{four orders of magnitude}.
%         \item Published at \textbf{ASPLOS 2024} and selected as \textbf{IEEE Top Pick 2025}.
%     \end{itemize}
%     \vspace{1em}
%     \item Project 2: \textbf{Temporal Activation and Real-Soft-Max Functions} (Lead, Under Review).
%     \vspace{0.5em}
%     \begin{itemize}
%         \item Developed the \textbf{first} general approximation framework in race logic that could apply to nonlinearity.
%         \item Built an efficient exact implementation of piecewise linear functions in race logic (e.g., ReLU) with \textbf{100\%} precision.
%         \item Efficiently approximated important classes of nonlinear functions in ML, such as Sigmoid and RealSoftMax, to \textbf{arbitrary precision} using only foundational temporal operators (less than 1\% error rate for all functions achieved with less than 0.08 pJ energy).
%         \item Uncovered \textbf{new Pareto-optimal} points that better balanced accuracy and energy-efficiency, via a novel energy-precision co-optimization that saved \textbf{32\%} and \textbf{24\%} energy for each function.
%     \end{itemize}
% \end{itemize}

\vspace{1em}

%-------------------------------------------------------------------
%	PUBLICATIONS SECTION
%-------------------------------------------------------------------

\section{\textbf{Selected Publications}}
\vspace{0.5em}

\cvitem{Preprint}{\textbf{\href{https://arxiv.org/abs/2404.12534}{Towards Large Language Models as Copilots for Theorem Proving in Lean}}}
\cvitem{}{\underline{\href{https://peiyang-song.github.io/}{Peiyang Song}}, \href{https://yangky11.github.io/}{Kaiyu Yang}, \href{https://www.eas.caltech.edu/people/anima}{Anima Anandkumar}}
\cvitem{}{\textit{NeurIPS Mathematical Reasoning and AI (MATH-AI) Workshop, 2023}}

\vspace{1em}

\cvitem{Preprint}{\textbf{\href{https://arxiv.org/abs/2502.17925}{LeanProgress: Guiding Search for Neural Theorem Proving via Proof Progress Prediction}}}
\cvitem{}{\href{https://hsz0403.github.io/}{Suozhi Huang}, \underline{\href{https://peiyang-song.github.io/}{Peiyang Song}}, \href{https://www.robertj1.com/}{Robert Joseph George}, \href{https://www.eas.caltech.edu/people/anima}{Anima Anandkumar}}
\cvitem{}{\textit{In submission, manuscript available upon request}}

\vspace{1em}

\cvitem{Preprint}{\textbf{Temporal Activation and Real-Soft-Max Functions}}
\cvitem{}{\underline{\href{https://peiyang-song.github.io/}{Peiyang Song}}, \href{https://www.linkedin.com/in/rhys-gretsch-462a951ab}{Rhys Gretsch}, \href{https://dl.acm.org/profile/81100206077}{Jeremy Lau}, and \href{https://www.arch.cs.ucsb.edu/prof-sherwood}{Timothy Sherwood}}
\cvitem{}{\textit{In submission, manuscript available upon request}}

\vspace{1em}

\cvitem{ICLR 2025}{\textbf{\href{https://arxiv.org/abs/2410.06209}{LeanAgent: Lifelong Learning for Formal Theorem Proving}}}
\cvitem{}{\href{https://www.linkedin.com/in/adarsh-kumarappan}{Adarsh Kumarappan}, \href{https://motiwari.com/}{Mo Tiwari}, \underline{\href{https://peiyang-song.github.io/}{Peiyang Song}}, \href{https://www.robertj1.com/}{Robert Joseph George}, \href{https://xiaocw11.github.io/}{Chaowei Xiao}, \href{https://www.eas.caltech.edu/people/anima}{Anima Anandkumar}}
\cvitem{}{\textit{International Conference on Learning Representations (ICLR) 2025}}

\vspace{1em}

\cvitem{EMNLP 2024}{\textbf{\href{https://arxiv.org/abs/2410.00988}{Creative and Context-Aware Translation of East Asian Idioms with GPT-4}}}
\cvitem{}{\href{https://www.linkedin.com/in/kenan-tang-1682a11a0/}{Kenan Tang}*, \underline{\href{https://peiyang-song.github.io/}{Peiyang Song}}*, \href{https://yaoqin1.github.io/}{Yao Qin}, \href{https://sites.cs.ucsb.edu/~xyan/}{Xifeng Yan} (* Equal Contribution)}
\cvitem{}{\textit{Findings of Empirical Methods in Natural Language Processing (EMNLP), 2024}}

\vspace{1em}

\cvitem{EMNLP 2024}{\textbf{\href{https://arxiv.org/abs/2409.15454}{In-Context Learning May Not Elicit Trustworthy Reasoning: A-Not-B Errors in Pretrained Language Models}}}
\cvitem{}{\href{https://pengruihan.github.io/}{Pengrui Han}*, \underline{\href{https://peiyang-song.github.io/}{Peiyang Song}}*, \href{https://haofeiyu.me/}{Haofei Yu}, \href{https://cs.stanford.edu/people/jiaxuan/}{Jiaxuan You} (* Equal Contribution)}
\cvitem{}{\textit{Findings of Empirical Methods in Natural Language Processing (EMNLP), 2024}}

\vspace{1em}

\cvitem{ASPLOS 2024}{\textbf{\href{https://dl.acm.org/doi/10.1145/3620665.3640395}{Energy Efficient Convolution with Temporal Arithmetic}}}
\cvitem{}{\href{https://www.linkedin.com/in/rhys-gretsch-462a951ab}{Rhys Gretsch}, \underline{\href{https://peiyang-song.github.io/}{Peiyang Song}}, \href{https://ireap.umd.edu/clark/staff/1448/Advait-Madhavan}{Advait Madhavan}, \href{https://dl.acm.org/profile/81100206077}{Jeremy Lau}, \href{https://www.arch.cs.ucsb.edu/prof-sherwood}{Timothy Sherwood}}
\cvitem{}{\textit{ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), 2024}; \textbf{IEEE Top Pick 2025}}

\vspace{1em}

\cvitem{NeurIPS 2023}{\textbf{\href{https://leandojo.org/}{LeanDojo: Theorem Proving with Retrieval-Augmented Language Models}}}
\cvitem{}{\href{https://yangky11.github.io/}{Kaiyu Yang}, \href{https://aidanswope.com/about}{Aidan Swope}, \href{https://minimario.github.io/}{Alex Gu}, \href{https://rchalamala.github.io/}{Rahul Chalamala}, \underline{\href{https://peiyang-song.github.io/}{Peiyang Song}}, \href{https://billysx.github.io/}{Shixing Yu}, \href{https://www.linkedin.com/in/saad-godil-9728353/}{Saad Godil}, \href{https://www.linkedin.com/in/ryan-prenger-18797ba1/}{Ryan Prenger}, \href{https://www.eas.caltech.edu/people/anima}{Anima Anandkumar}}
\cvitem{}{\textit{Neural Information Processing Systems (NeurIPS), 2023}, \textbf{Oral presentation}}

\vspace{1em}

%-------------------------------------------------------------------
%	AWARDS & HONORS SECTION
%-------------------------------------------------------------------

\section{\textbf{Awards \& Honors}}
\vspace{0.5em}

\cvitem{2/2025}{\textbf{IEEE Top Pick}}

\vspace{0.5em}

\cvitem{8/2023}{\textbf{Early Research Scholarship}}

\vspace{0.5em}

\cvitem{4/2023}{\textbf{Caltech SURF Award}}

\vspace{0.5em}

\cvitem{9/2022}{\textbf{UCSB Creative Studies Honors}}

\vspace{1em}

%-------------------------------------------------------------------
%	MEDIA SECTION
%-------------------------------------------------------------------

\section{\textbf{Selected Media}}
\vspace{0.5em}

\cvitem{2024}{\textbf{\href{https://www.scientificamerican.com/article/mathematicians-newest-assistants-are-artificially-intelligent/}{Mathematicians’ Newest Assistants Are Artificially Intelligent}}}
\cvitem{}{\textit{Scientific American}}

\cvitem{2024}{\textbf{\href{https://www.marktechpost.com/2024/10/11/leanagent-the-first-life-long-learning-agent-for-formal-theorem-proving-in-lean-proving-162-theorems-previously-unproved-by-humans-across-23-diverse-lean-mathematics-repositories/}{LeanAgent: The First Life-Long Learning Agent for Formal Theorem Proving in Lean}}}
\cvitem{}{\textit{MarkTechPost}}

\cvitem{2024}{\textbf{\href{https://www.marktechpost.com/2024/07/30/lean-copilot-an-ai-tool-that-allows-large-language-models-llms-to-be-used-in-lean-for-proof-automation/}{Lean Copilot: An AI Tool that Allows Large Language Models (LLMs) to be used in Lean for Proof Automation}}}
\cvitem{}{\textit{MarkTechPost}}

\vspace{0.5em}

\cvitem{2023}{\textbf{\href{https://www.marktechpost.com/2023/07/01/can-llms-generate-mathematical-proofs-that-can-be-rigorously-checked-meet-leandojo-an-open-source-ai-playground-with-toolkits-benchmarks-and-models-for-large-language-models-to-prove-formal-theore/}{Can LLMs Generate Mathematical Proofs that can be Rigorously Checked?}}}
\cvitem{}{\textit{MarkTechPost}}

\vspace{1em}

%-------------------------------------------------------------------
%	LANGUAGES SECTION
%-------------------------------------------------------------------

\section{\textbf{Languages}}
\vspace{0.5em}

\cvitem{Programming}{Python, C++, Lean 4, Java, C, PASCAL, OCaml, C\#}

\vspace{0.5em}

\cvitem{Natural}{English (TOEFL 117/120), Mandarin (Native)}

\vspace{1em}

%-------------------------------------------------------------------
%	TALKS SECTION
%-------------------------------------------------------------------

\section{\textbf{Invited Talks \& Tutorials}}
\vspace{0.5em}

\cvitem{}{\textbf{Tutorial: Neuro-Symbolic Theorem Proving with Lean}}
\vspace{0.5em}
\cvitem{9/2024}{3rd Neuro-Symbolic AI Summer School (NSSS)}

\vspace{1em}

\cvitem{}{\textbf{Towards An AI Mathematician}}
\vspace{0.5em}
\cvitem{12/2023}{UC Santa Barbara NLP Lab}
\cvitem{11/2023}{CCS Research \& Creative Activities Conference (RACA-CON)}
\cvitem{8/2023}{Caltech SURF Seminar Day}

\vspace{1em}

%-------------------------------------------------------------------
%	ACADEMIC SERVICES SECTION
%-------------------------------------------------------------------

\section{\textbf{Academic Services}}
\vspace{0.5em}

\cvitem{\textbf{Reviewer}}{Conference on Neural Information Processing Systems (NeurIPS)}
\cvitem{}{International Conference on Learning Representations (ICLR)}
\cvitem{}{Association for Computational Linguistics (ACL) Rolling Review}
\cvitem{}{Annual Meeting of the Association for Computational Linguistics (ACL)}
\vspace{0.5em}
\cvitem{}{NeurIPS Mathematical Reasoning and AI (MATH-AI) Workshop}
\cvitem{}{NeurIPS Workshop on Behavioral Machine Learning}
\cvitem{}{ICLR {VerifAI}: AI Verification in the Wild Workshop}
\cvitem{}{ICLR Workshop on Representational Alignment (Re-Align)}
\cvitem{}{ICML Workshop on LLMs and Cognition}

\vspace{1em}

\end{document}
